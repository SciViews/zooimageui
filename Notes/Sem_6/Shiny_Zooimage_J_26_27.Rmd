---
title: "Shiny_Zooimage_J_26"
output: html_document
---

Fichier de notes du jour 26, Lundi 07 Mars 2022.    Présentiel.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

=== Choses à faire update ===

1) Données sur le disque : Qu'est-ce qu'on peut faire avec l'app Shiny ? Accès des données ? Ailleurs sur le disque ? (vdd)
  => J'ai déjà beaucoup regardé à ça, et rien trouvé de spécial...

2) Golem => modularisation
  => J'ai déjà regardé au fonctionnement de ça également : Je sais comment faire à partir d'un nouveau projet.

3) Machine learning : classification
  => J'y ai également regardé un peu, les débuts de la modélisation.

4) Cahier des charges (définition la plus complète et détaillée de ce que le travail doit faire) \
- fonctionnalités (ex : traitements local / serveur) \
- implémentation (comment ça fonctionne) (ex : accès des données, ...) \
- interface (MOCK : ex, draw.io) \
- timing \
- tests/maintenances \

=============================


Pour reprendre dans mon apprentissage de la modélisation dans ZooImage : \
Certaines de ces commandes ne peuvent pas être exécutées directement dans le chunk car sinon elle ne s'appliquent pas au long terme. (les setups) (old_dir <- setwd(rootpath) principalement) \

```{r}
rootpath <- "/home/rstudio/shared/zooimage-ui/ZI_Tests/"

smps <- c("./data/Samples/MTPS.2004-10-20.V5.zidb",
          "./data/Samples/MTLG.2005-05-24.H1.zidb")
library(zooimage)

old_stringsAsFactors <- getOption("stringsAsFactors")
options(stringsAsFactors = TRUE)

old_dir <- setwd(rootpath)

train <- getTrain("./data/Training set")
train$Class <- factor(train$Class, levels = basename(attr(train, "path")) )

classif <- ZIClass(Class ~ ., train, method = "mlRforest", calc.vars = calcVars,
    ntree = 200, cv.k = 10) # Note than method, calc.vars & cv.k are default values
classif


```

Les derniers tests effectués à partir du manuel :

```{r}
names(train) # Original variables
names(traincalc <- calcVars(train)) # Variables as seen be classifier
```

1) Première étape : calcul et sélection des variables \

Procédure de choix d'une variable, en se basant sur l'indice de Gini, calculé avec l'algorithme de forêt aléatoire, pour sélectionner seulement les prédicteur les plus discriminants.

!!!!! Possibilité de choix !!!!!

```{r}
require(randomForest)
# Importance of the predictors
Imp <- classif$importance
varImpPlot(classif, n.var = nrow(Imp))
# Importance of preditor variables as measured by the mean decrease of the Gini index for the random forest classifier classif.
Threshold <- 30
abline(v = Threshold, lty = 2)
# Drop variables with low Gini decrease
VarsToDrop <- rownames(Imp)[Imp < Threshold]
```

La liste des prédicteurs que on veut utiliser dans Zooimage est spécifié dans l'option ZI.dropVars de cette manière :

```{r}
Imp <- classif$importance
Threshold <- 30
VarsToDrop <- rownames(Imp)[Imp < Threshold]
options(ZI.dropVars = VarsToDrop)
# Now, 'Min' is drop from the dataset
names(traincalc <- calcVars(train, drop.vars = VarsToDrop)) # ou getOption("ZI.dropVars")
```

Il faut faire attention que le options ne suffit pas. ici, il faut préciser dans le calcVars() l'option drop.vars = "" afin de préciser quelles variables calculées on souhaite éliminer. (soit en prenant les valeurs de VarsToDrop ou en allant chercher avec getOption()) \

Concrêtement, on pourrait remplacer la fonction calcVars afin qu'elle calcule d'autre variables qui nous intéresse, d'autre prédicteurs.

!!!!! Possibilité de choix !!!!!

Une autre étape utile consiste à choisir des meilleurs paramètres pour l'algorithme de classification. Par exemple, pour "random forest" on peut changer le nombre d'arbres à utiliser. \

!!!!! Possibilité de choix !!!!!

```{r}
plot(classif$err.rate[, "OOB"], type = "l", xlab = "trees", ylab = "Error")
abline(v = 200, lty = 2)
```

En fait dans ce cas, le résultat est pas très utile ni intéressant car on avait déjà set le nombre d'abres à 200. Mais l'idée est que si le nombre d'abres était 500 par exemple, on verrait que la courbe devient très linéaire à partir de 200. Donc on a gardé la valeur de 200 arbres. \


Voir ce qu'est err.rate dans classif :
```{r}
names(classif)
classif$err.rate
classif$err.rate[,"OOB"]
```

```{r}
classifrf <- mlearning(Class ~ ., data = traincalc,
    method = "mlRforest", ntree = 200)
```
=> !!! mlearning similaire à ZIClass ? non, c'est une étape de ZIClass 


2) Deuxième étape : Création du classifieur \


3) Troisième et dernière étape réalisé lors de la création d'un objet ZIClass : \
Validation croisée X-Fold sur le dataframe avec ses nouvelles variables (en utilisant le même algorithme de machine learning) \

Ceci est l'équivalent : \
```{r}
# 10-fold cross-validation set by using cv.k = 10
classifcv <- predict(classifrf, method = "cv", cv.k = 10)
```


En fait dans le bloc suivant on a un résumé :
```{r}
# Partie création d'un classifieur, sur base de ZooImage, tout est fait en une fonction

classif <- ZIClass(Class ~ ., data = train, method = "mlRforest", calc.vars = calcVars,
    ntree = 200, cv.k = 10) # Note than method, calc.vars & cv.k are default values
classif

# Partie création du même classifieur, mais en passant par d'autres fonctions, en faisant chaque étape

## The various steps involved here are:
## 1) Calculation of derived/transformed variables, and also, possibly,
##    selection of variables is done by calcVars() (or your awn function)
names(train) # Original variables
names(traincalc <- calcVars(train)) # Variables as seen be classifier
## 2) Train a classifier on transformed data frame
classifrf <- mlRforest(Class ~ ., data = traincalc, ntree = 200)
## 3) Apply x-fold cross-validation on the same training set with the choosen
##    algorithm (here, 10-fold cross-validation)
classifcv <- predict(classifrf, method = "cv", cv.k = 10)
```

En fait dans la fonction ZIClass de ZooImage, on appelle toute une série de fonctions d'autres packages comme mlearning. \
Il y a différentes méthodes/stratégies pour faire la "10-Fold cross validation" en utilisant un échantillon stratifié. \
De base on utilise cv qui est l'équivalent de cvpredict() (utilisation de errorest(), du package ipred) \
On peut voir d'autres stratégies pour le faire via ?cvpredict et ?errorest \

!!!!! Possibilité de choix !!!!!


En résumé, on peut faire l'objet ZIClass soit en faisant toutes les étapes soit en utilisant la fonction ZIClass de ZooImage \

```{r}
classif <- ZIClass(Class ~ ., data = train, method = "mlRforest", calc.vars = calcVars, ntree = 200, cv.k = 10)
```

Il existe bien d'autres algorithmes de machine learning disponibles dans mlearning : \
- "linear discriminant analysis" \
- "quadratic linear discriminant analysis" \
- "learning vector quantization" \
- "neural network" \
- "support vector machine" \
- "naïve Bayes" \
- "random forest" \

Les objects créés par mlearning sont aussi des "interfaces" utilisant des algorithmes provenant d'autres packages. \

(MASS, class, nnet, e1071, randomForest, ...)



============== Model Evaluation ==============

Les "ROC curves" sont un bon outil pour évaluer des classifieur binaires.

Mais le problème c'est qu'elles ne sont pas adaptables facilement à des problèmes "multi-class", c'est à dire surtout quand on a un très grand nombre de classes. \
Dans le cas d'un grand nombre de classes, la matrice de confusion, et les statistiques basées sur les matrices de confusion comme le F-score sont les meilleurs indicateurs.

Le package mlearning propose des objets de type "confusion", qui possède des méthodes utiles comme print(), summary(), plot() (avec quatre types différents) pour évaluer des classifieurs.

```{r}
# All proposed statistics descriptors
summary(classif)
```

Le summary() d'un object ZIClass est identique au summary(confusion(ZIClass))

Dans ce cas, cela utilise toujours des prédictions "cross-validated", mais dans le cas de certains algorithmes, comme pour "random forest", on pourrait utiliser les "out-of-bag" aussi. \
On peut également choisir les statistiques que l'on souhaite. \

!!!!! Possibilité de choix !!!!!

```{r}
# Get selected statistics only
summary(classif, type = c("Fscore", "Recall", "Precision"))[1:3,]
# Only the output for the three first classes is printed
```

On peut construire l'objet de confusion directement (de façon explicite) si on veut accéder et inspecter plus précisémment la matrice de confusion.

```{r}
conf <- confusion(classif)
conf
```

Les objets de type "confusion" héritent de la "standard R table" et propose leurs summary(), print(), et plot(), spécifiques.

```{r}
# type = 'image' is default and thus facultative
plot(conf, type = "image")
```

Il faut faire attention que par défaut, la matrice de confusion est affichée avec les classes triées en fonction de leurs taux mutuels de faux positifs et de faux négatifs. \
Cela permet ainsi de rapprocher au maximum les classes avec les plus grandes "confusion".

# Détail : Vraiment important ?? \
Le tri est réalisé en utilisant le clustering hierarchique ("hierarchical clustering", avec fonction hclust()) sur une matrice symétrique, avec une moyenne des faux positifs et faux négatifs en dehors de la diagonale. \
(et par défaut le "Wald aggregation criterion")

On peut voir le dendrogamme de ce clustering : \

```{r}
plot(conf, type = "dendrogram")
```
# Détail

Si on souhaite, dans le cas de random forest, utiliser les prédictions out-of-bag, on pourrait faire : \
```{r}
## In the specific case of random forest, you could decide as well to use the
## out-of-bag prediction at this stage:
confoob <- confusion(classif, predict(classif, method = "oob"))
## If you combine this, with ZIClass(...., cv.k = NULL), you short circuit
## the time-consumming cross-validation step on the creation of your ZIClass
## object, and you can fine-tune it faster. However, out-of-bag is not
## available for all machine learning algorithms, and it is also perhaps
## not as reliable as stratified cross-validation
```

On peut ensuite faire différents types d'affichages, comme par exemple, pour représenter la performance de notre classifieur : \
(related to recall versus precision for each class, or its combination as F1-score are also available) \

```{r}
plot(conf, type = "barplot")
plot(conf, type = "stars")
```

Pour comparer deux classifieurs : Il y a un graphe pour !

(On crée un autre classifieur méthode différente, pour tester)
A partir d'un exemple : On va créer un classifieur de type "support vector machine classifier" avec "linear kernel" à partir du même training set. \
On va ensuite le comparer à random forest, classe par classe, en fonction du "recall" et de la "précision". \

```{r}
classif2 <- ZIClass(Class ~ ., train, method = "mlSvm", kernel = "linear")
classif2
summary(classif2)
conf2 <- confusion(classif2)
conf2
plot(conf2, type = "barplot")
plot(conf2)
plot(conf, conf2)
```

Pour un petit rappel des matrices de confusion : https://kobia.fr/classification-metrics-precision-recall/ \

Les statistiques basées sur les matrices de confusions sont dépendantes des proportions relatives d'items dans chaque classe, c'est donc important de définir des bons à prioris. \
Les communautés de plancton peuvent changer drastiquement. Les groupes taxonomiques dominant une communauté peuvent être différent en fonction des différents endroits (géographiques) et moments (temps). \
De plus, le nombre d'individus dans le training set tend à être plus équitable ("balanced"), avec un sur-échantillonage des groupes plus rares, pour avoir assez de vignettes pour faire l'apprentissage. \
Cela signifie que les probabilités précédentes ont besoin d'être ajustées pour refléter les proportions relatives de chaque classe dans une situation pratique. \
Cela peut être fait très facilement avec l'objet "confusion". \

Par exemple, on pourrait inspecter les performances de l'algorithme random forest, avec un échantillon réalistique qui contient une moitié de particules non vivantes (neige marine ou débris ou fibres), et une communauté de zooplancton dominée par les "calanoid copepods" et les "chaetognaths" de cette manière :

```{r}
priors <- rep(20, 25) # On crée un priors avec 25 fois la valeur 20
names(priors) <- rownames(conf) # On ajout les noms des classes
priors[c("Calanoida", "Chaetognatha")] <- c(300, 100) # On passe le nombres des zooplanctons sélectionnés à une valeur représentative
priors[c("debris", "fiber", "marine snow")] <- 266 # On passe le nombre des particules non vivantes à 266
priors

prior(conf) <- priors # On modifie les valeurs de priors pour notre matrice de confusion
summary(conf, type = c("Fscore", "Recall", "Precision"))[1:3, ] # certaines métriques, 3 lignes
# Pour reset les priors dans le training set :
prior(conf) <- NULL
```

Sinon, il existe une meilleur méthode : \
Utiliser un test séparé, fait à partir d'un échantillon représentatif. ("test set") \
Voilà ce qu'on a dans notre premier échantillon dans le dataset de l'exemple : \

```{r}
old_wd <- getwd()
setwd("/home/rstudio/shared/zooimage-ui/ZI_Tests/")
prepareTest(file.path(rootpath, "data", "_test"), smps[1], template = classif)
# Maintenant on a un dossier de test, dont on est sensé trier les vignettes, mais on va supposé que c'est fait :
```
```{r}
test <- getTest(file.path(rootpath,"data", "Test sets", "MTPS.2004-10-20.V5"))
# Création et étude de la matrice de confusion
conf3 <- confusion(predict(classif, test), test$Class)
conf3
```

(La formule utilisé pour créer la matrice de confusion 3 m'est plus obscure ...) \

La moitié des particules dans notre échantillon de test sont mal classifiées ! \
=> C'est à cause de la présence des particules qui ne sont pas visuellement reconnaissables dans les vignettes (elles augmentent le taux d'erreur) \

De tels scores mènent à des résultats inutilisables pour les biologistes ou écologistes. Sauf si ils réduisent la fraction de mauvaise classification en ayant réalisé une sorte de "validation manuelle" dans la littérature du plancton. \
Cela consiste à faire une correction manuelle de la classification réalisée automatiquement. \
On le fait de manière très similaire à ce qu'on fait quand on trie les vignettes au début, mais en introduisant d'abord les vignettes dans les différents dossiers en fonction de leur classification automatique. \

Voici un exemple (échantillon séparé, avec son set de validation) \
On utilisera alors l'argument classes = classif (dans prepareTest()) pour activer le pré-tri des vignettes en utilisant le classifieur. \

```{r}
old_wd <- getwd()
setwd("/home/rstudio/shared/zooimage-ui/ZI_Tests/")
prepareTest(file.path(rootpath, "data", "_test2"), smps[2], template = classif, classes = classif)
## Maintenant on peut aller voir les dossiers et sous-dossiers et re-classer les mauvaises vignettes
## On va dire que c'est le bon résultat ...
```
```{r}
test2 <- getTest(file.path(rootpath, "data", "Test sets", "MTLG.2005-05-24.H1"))
# On crée et on étudie la matrice de confusion
conf4 <- confusion(predict(classif, test2), test2$Class)
conf4
```

Et oui, c'est plus rapide de vérifier que la classification est bonne et ne changer que ce qui est mauvais, que de classifier l'entièreté de l'échantillons à la main. \
Voilà pourquoi cette pratique reste utile et nécessaire. Cela aide le machine learning à mieux se dérouler. \
Il y a un gain, un profit significatif à traiter les échantillons de plancton de cette manière. (dans la discussion on parle d'idées ou d'améliorations) \





Termes à mieux comprendre :
 - F-score
 - Gini criterion
 - Wald aggregation criterion
 - valeurs "cross-validated" et celles "out-of-bag"
 - Priors ?
 => Être sur de bien comprendre :
 - Recall
 - Precision
 - Accuracy


Questions : 

- Si bien compris : ZIClass utilise mlearning et ses fonctions, ainsi que d'autres package pour tout faire d'un coup.
- Détails, importants ?
- "Wald aggregation criterion" ??


